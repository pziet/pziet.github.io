<!DOCTYPE html>
<html>

<head>
    <link rel="stylesheet" href="styles.css">
    <title>Papers</title>
    <script src="script.js"></script>
</head>
<div id="menu-placeholder"></div>
<div class="content">

    <body>
        <h2>Publications</h2>
        <a href="https://scholar.google.com/citations?hl=en&user=IyiPlGgAAAAJ" target="_blank">Google Scholar</a>
        <ol>
            <li> <strong>Zietkiewicz, P</strong>, <a href="https://ikosmidis.com/" target="_blank">Kosmidis, I</a>
                (2024). <em>Bounded-memory adjusted
                    scores estimation in
                    generalized linear models with large data sets</em>. Statistics and Computing, 34, 138. <a
                    href="https://link.springer.com/article/10.1007/s11222-024-10447-z" target="_blank">DOI</a> <a
                    href="https://arxiv.org/pdf/2307.07342" target="_blank">ArViV</a> <a
                    href="https://github.com/ikosmidis/bigbr-supplementary-material" target="_blank">Supplementary
                    Material</a>:</li>
            <p>The widespread use of maximum Jeffreys'-prior penalized likelihood in binomial-response generalized
                linear models, and in logistic regression, in particular, are supported by the results of Kosmidis and
                Firth (2021, Biometrika), who show that the resulting estimates are always finite-valued, even in cases
                where the maximum likelihood estimates are not, which is a practical issue regardless of the size of the
                data set. In logistic regression, the implied adjusted score equations are formally bias-reducing in
                asymptotic frameworks with a fixed number of parameters and appear to deliver a substantial reduction in
                the persistent bias of the maximum likelihood estimator in high-dimensional settings where the number of
                parameters grows asymptotically as a proportion of the number of observations. In this work, we develop
                and present two new variants of iteratively reweighted least squares for estimating generalized linear
                models with adjusted score equations for mean bias reduction and maximization of the likelihood
                penalized by a positive power of the Jeffreys-prior penalty, which eliminate the requirement of storing
                <em>O(n)</em> quantities in memory, and can operate with data sets that exceed computer memory or even
                hard drive
                capacity. We achieve that through incremental QR decompositions, which enable IWLS iterations to have
                access only to data chunks of predetermined size. Both procedures can also be readily adapted to fit
                generalized linear models when distinct parts of the data is stored across different sites and, due to
                privacy concerns, cannot be fully transferred across sites. We assess the procedures through a real-data
                application with millions of observations.
            </p>

            <li><a href="https://ikosmidis.com/" target="_blank">Kosmidis, I</a>, <strong>Zietkiewicz, P</strong>
                (2025).
                <em>Jeffreys-prior penalty for high-dimensional
                    logistic regression: A conjecture about aggregate bias.</em> Statistical Science (accepted). <a
                    href="https://arxiv.org/pdf/2311.11290" target="_blank">ArXiV</a> <a
                    href="https://github.com/ikosmidis/mJPL-conjecture-supplementary" target="_blank">Supplementary
                    Material</a>:
            </li>
            <p>Firth (1993, Biometrika) shows that the maximum Jeffreys' prior penalized likelihood estimator in
                logistic regression has asymptotic bias decreasing with the square of the number of observations when
                the number of parameters is fixed, which is an order faster than the typical rate from maximum
                likelihood. The widespread use of that estimator in applied work is supported by the results in Kosmidis
                and Firth (2021, Biometrika), who show that it takes finite values, even in cases where the maximum
                likelihood estimate does not exist. Kosmidis and Firth (2021, Biometrika) also provide empirical
                evidence that the estimator has good bias properties in high-dimensional settings where the number of
                parameters grows asymptotically linearly but slower than the number of observations. We design and carry
                out a large-scale computer experiment covering a wide range of such high-dimensional settings and
                produce strong empirical evidence for a simple rescaling of the maximum Jeffreys' prior penalized
                likelihood estimator that delivers high accuracy in signal recovery in the presence of an intercept
                parameter. The rescaled estimator is effective even in cases where estimates from maximum likelihood and
                other recently proposed corrective methods based on approximate message passing do not exist.</p>
        </ol>
    </body>
</div>

</html>